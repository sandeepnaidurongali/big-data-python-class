{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am creating all inputs for scrapy and crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandeep\\big-data-python-class\\Homeworks\\Homework8\\code\n"
     ]
    }
   ],
   "source": [
    "cd code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below creates requirements for scrapy and crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while terminating subprocess (pid=7948): Unsupported signal: 2\n"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "scrapy startproject tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error 2] The system cannot find the file specified: u'tutorial'\n",
      "C:\\Users\\Sandeep\\big-data-python-class\\Homeworks\\Homework8\\code\\tutorial\\tutorial\\spiders\n"
     ]
    }
   ],
   "source": [
    "cd tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 6.1.7601]\r\n",
      "Copyright (c) 2009 Microsoft Corporation.  All rights reserved.\r\n",
      "\r\n",
      "C:\\Users\\Sandeep\\big-data-python-class\\Homeworks\\Homework8\\code\\tutorial>scrapy genspider nba nba.com\n",
      "Created spider 'nba' using template 'basic' in module:\r\n",
      "  tutorial.spiders.nba\r\n",
      "\r\n",
      "C:\\Users\\Sandeep\\big-data-python-class\\Homeworks\\Homework8\\code\\tutorial>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "scrapy genspider nba nba.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandeep\\big-data-python-class\\Homeworks\\Homework8\\code\\tutorial\\tutorial\n"
     ]
    }
   ],
   "source": [
    "cd tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class TutorialItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandeep\\big-data-python-class\\Homeworks\\Homework8\\code\\tutorial\\tutorial\\spiders\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\Sandeep\\big-data-python-class\\Homeworks\\Homework8\\code\\tutorial\\tutorial\\spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I take the initially corresponding url's of www.nba.com and will start crawling web page urls into different categories for future usage of numpys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load nba.py\n",
    "import scrapy\n",
    "\n",
    "class nbaItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    resp = scrapy.Field()\n",
    "class NbaSpider(scrapy.Spider):\n",
    "    name = 'ba'\n",
    "    allowed_domains = ['amazon.com']\n",
    "    start_urls = ['http://www.nba.com/scores#/','https://stats.nba.com/schedule/','http://www.nba.com/news','https://stats.nba.com/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        res = scrapy.Selector(response)\n",
    "        titles = res.xpath('//ul/li')\n",
    "        items = []\n",
    "        for title in titles:\n",
    "            item = nflItem()\n",
    "            item[\"title\"] = title.xpath(\"a/text()\").extract()\n",
    "            item[\"link\"] = title.xpath(\"a/@href\").extract()\n",
    "            item[\"resp\"] = response\n",
    "            if item[\"title\"] != []:\n",
    "                items.append(item)\n",
    "\n",
    "        return items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will crawl all the webpages recursively going into deeper url's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 6.1.7601]\r\n",
      "Copyright (c) 2009 Microsoft Corporation.  All rights reserved.\r\n",
      "\r\n",
      "C:\\Users\\Sandeep\\big-data-python-class\\Homeworks\\Homework8\\code\\tutorial\\tutorial\\spiders>scrapy crawl nba\n",
      "\r\n",
      "C:\\Users\\Sandeep\\big-data-python-class\\Homeworks\\Homework8\\code\\tutorial\\tutorial\\spiders>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-12 18:07:45 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: tutorial)\r\n",
      "2018-11-12 18:07:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 2.7.15 |Anaconda, Inc.| (default, May  1 2018, 18:37:09) [MSC v.1500 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\runpy.py\", line 174, in _run_module_as_main\r\n",
      "    \"__main__\", fname, loader, pkg_name)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\runpy.py\", line 72, in _run_code\r\n",
      "    exec code in run_globals\r\n",
      "  File \"C:\\Users\\Sandeep\\Anaconda2\\Scripts\\scrapy.exe\\__main__.py\", line 9, in <module>\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\cmdline.py\", line 150, in execute\r\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\cmdline.py\", line 90, in _run_print_help\r\n",
      "    func(*a, **kw)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\cmdline.py\", line 157, in _run_command\r\n",
      "    cmd.run(args, opts)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 57, in run\r\n",
      "    self.crawler_process.crawl(spname, **opts.spargs)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\crawler.py\", line 170, in crawl\r\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\crawler.py\", line 198, in create_crawler\r\n",
      "    return self._create_crawler(crawler_or_spidercls)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\crawler.py\", line 202, in _create_crawler\r\n",
      "    spidercls = self.spider_loader.load(spidercls)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\spiderloader.py\", line 71, in load\r\n",
      "    raise KeyError(\"Spider not found: {}\".format(spider_name))\r\n",
      "KeyError: 'Spider not found: nba'\r\n"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "scrapy crawl nba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving all crawl pages to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 6.1.7601]\r\n",
      "Copyright (c) 2009 Microsoft Corporation.  All rights reserved.\r\n",
      "\r\n",
      "C:\\Users\\Sandeep\\big-data-python-class\\Homeworks\\Homework8\\code\\tutorial\\tutorial\\spiders>scrapy crawl nba -o results.csv -t csv\n",
      "\r\n",
      "C:\\Users\\Sandeep\\big-data-python-class\\Homeworks\\Homework8\\code\\tutorial\\tutorial\\spiders>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-12 18:08:19 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: tutorial)\r\n",
      "2018-11-12 18:08:19 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 2.7.15 |Anaconda, Inc.| (default, May  1 2018, 18:37:09) [MSC v.1500 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP1\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\runpy.py\", line 174, in _run_module_as_main\r\n",
      "    \"__main__\", fname, loader, pkg_name)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\runpy.py\", line 72, in _run_code\r\n",
      "    exec code in run_globals\r\n",
      "  File \"C:\\Users\\Sandeep\\Anaconda2\\Scripts\\scrapy.exe\\__main__.py\", line 9, in <module>\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\cmdline.py\", line 150, in execute\r\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\cmdline.py\", line 90, in _run_print_help\r\n",
      "    func(*a, **kw)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\cmdline.py\", line 157, in _run_command\r\n",
      "    cmd.run(args, opts)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 57, in run\r\n",
      "    self.crawler_process.crawl(spname, **opts.spargs)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\crawler.py\", line 170, in crawl\r\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\crawler.py\", line 198, in create_crawler\r\n",
      "    return self._create_crawler(crawler_or_spidercls)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\crawler.py\", line 202, in _create_crawler\r\n",
      "    spidercls = self.spider_loader.load(spidercls)\r\n",
      "  File \"c:\\users\\sandeep\\anaconda2\\lib\\site-packages\\scrapy\\spiderloader.py\", line 71, in load\r\n",
      "    raise KeyError(\"Spider not found: {}\".format(spider_name))\r\n",
      "KeyError: 'Spider not found: nba'\r\n"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "scrapy crawl nba -o results.csv -t csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<200 https://stats.nba.com/schedule/>\n",
      "['https://stats.nba.com/schedule/']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "temp =[]\n",
    "data=pd.read_csv(\"results.csv\")\n",
    "data['link']='http://www.nba.com'+data['link']\n",
    "x=data['resp'][0]\n",
    "print x\n",
    "temp.append((x.split()[-1]).split('>')[-2])\n",
    "print temp\n",
    "for i in range(len(data)):\n",
    "    if data['resp'][i] == x:\n",
    "        temp.append(data['link'][i])\n",
    "    else:\n",
    "        x=data['resp'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_items=list(pd.DataFrame(temp)[0].unique())\n",
    "links = []\n",
    "length = len(temp)\n",
    "for i, val in enumerate(temp):\n",
    "    if i < length-1:\n",
    "        links.append((temp[i], temp[i+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = pd.DataFrame(index=nba_items, columns=nba_items)\n",
    "m = n.replace(np.NaN, 0)\n",
    "for i in links:\n",
    "    m.loc[i] = 1.0\n",
    "arr = np.array(m)\n",
    "v = arr.sum(axis=1)\n",
    "res = arr/v[:, np.newaxis]\n",
    "sophist_matrix = np.nan_to_num(res)\n",
    "sophist_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "def pageRank(G, s = .85, maxerr = .001):\n",
    "    \"\"\"\n",
    "    Computes the pagerank for each of the n states.\n",
    "    Used in webpage ranking and text summarization using unweighted\n",
    "    or weighted transitions respectively.\n",
    "    Args\n",
    "    ----------\n",
    "    G: Stochastic Matrix\n",
    "       \n",
    "    Kwargs\n",
    "    ----------\n",
    "    s(theta): probability of following a transition. 1-s probability of teleporting\n",
    "       to another state. Defaults to 0.85\n",
    "    maxerr: if the sum of pageranks between iterations is bellow this we will\n",
    "            have converged. Defaults to 0.001\n",
    "    \"\"\"\n",
    "    n = G.shape[0]\n",
    "\n",
    "    # transform G into markov matrix M\n",
    "    M = csc_matrix(G,dtype=np.float)\n",
    "    rsums = np.array(M.sum(1))[:,0]\n",
    "    ri, ci = M.nonzero()\n",
    "    M.data /= rsums[ri]\n",
    "\n",
    "    # bool array of sink states\n",
    "    sink = rsums==0\n",
    "\n",
    "    # Compute pagerank r until we converge\n",
    "    ro, r = np.zeros(n), np.ones(n)\n",
    "    while np.sum(np.abs(r-ro)) > maxerr:\n",
    "        ro = r.copy()\n",
    "        # calculate each pagerank at a time\n",
    "        for i in xrange(0,n):\n",
    "            # inlinks of state i\n",
    "            Ii = np.array(M[:,i].todense())[:,0]\n",
    "            # account for sink states\n",
    "            Si = sink / float(n)\n",
    "            # account for teleportation to state i\n",
    "            Ti = np.ones(n) / float(n)\n",
    "\n",
    "            r[i] = ro.dot( Ii*s + Si*s + Ti*(1-s) )\n",
    "\n",
    "    # return normalized pagerank\n",
    "    return r/sum(r)\n",
    "\n",
    "#print pageRank(pd.DataFrame(pagerank(sophist_matrix)),s=.86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0L, 1L, 130L, 143L, 308L, 197L, 314L, 300L, 136L, 41L]\n"
     ]
    }
   ],
   "source": [
    "Rank_val=pd.DataFrame(pageRank(sophist_matrix,s=.86))\n",
    "#Rank_val = pd.DataFrame(pagerank(sophist_matrix))\n",
    "Rank_sort = (Rank_val.sort_values(0)).head(10)\n",
    "Rank_lt = list(Rank_sort.index)\n",
    "print Rank_lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 page URLs:\n",
      "https://stats.nba.com/schedule/\n",
      "http://www.nba.com/player/{{ player.id }}/\n",
      "http://www.nba.com//nbatickets.nba.com\n",
      "http://www.nba.com/players/defensive-impact/\n",
      "http://www.nba.com/allstar2018/saturday/\n",
      "http://www.nba.com/teams/hustle-leaders/\n",
      "http://www.nba.com/teams/shots-dribbles/\n",
      "http://www.nba.com/teams/opponent-shots-general/\n",
      "http://www.nba.comhttp://www.nba.com/fantasy\n",
      "http://www.nba.com/team/{{ team.id }}/onoffcourt-misc/\n"
     ]
    }
   ],
   "source": [
    "url = pd.DataFrame(nba_items)\n",
    "print \"Top 10 page URLs:\"\n",
    "for i in Rank_lt:\n",
    "    print url[0][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hits Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits(A):\n",
    "    n= len(A)\n",
    "    Au= dot(transpose(A),A)\n",
    "    Hu = dot(A,transpose(A))\n",
    "    a = ones(n);\n",
    "    h = ones(n)\n",
    "    #print a,h\n",
    "    for j in range(5):\n",
    "        a = dot(a,Au)\n",
    "        a= a/sum(a)\n",
    "        h = dot(h,Hu)\n",
    "        h = h/ sum(h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_val = pd.DataFrame(hits(sophist_matrix))\n",
    "hit_sort = (hit_val.sort_values(0, ascending=False)).head(10)\n",
    "hit_lt = list(hit_sort.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 page URLs:\n",
      "http://www.nba.com/teams/shooting/\n",
      "http://www.nba.com/teams/clutch-opponent/\n",
      "http://www.nba.comhttp://www.nba.com/enebea/index.html\n",
      "http://www.nba.com/players/isolation/?OD=defensive\n",
      "http://www.nba.com/schedule/summerleague/\n",
      "http://www.nba.com/teams/\n",
      "http://www.nba.com/lineups/opponent/\n",
      "http://www.nba.com/teams/isolation/?OD=defensive\n",
      "http://www.nba.com/teams/paint-touch/\n",
      "http://www.nba.com/players/boxscores/\n"
     ]
    }
   ],
   "source": [
    "url = pd.DataFrame(nba_items)\n",
    "print \"Top 10 page URLs:\"\n",
    "for i in hit_lt:\n",
    "    print url[0][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
